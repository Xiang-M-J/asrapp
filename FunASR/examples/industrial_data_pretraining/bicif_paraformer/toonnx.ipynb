{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Some standard imports\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "\n",
    "from funasr import AutoModel\n",
    "\n",
    "model = AutoModel(\n",
    "    model=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n",
    "    # vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n",
    "    # punc_model=\"iic/punc_ct-transformer_cn-en-common-vocab471067-large\",\n",
    "    # spk_model=\"iic/speech_campplus_sv_zh-cn_16k-common\",\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ae6e05aa1c24f7b5",
   "metadata": {},
   "source": [
    "bimodel = model.model\n",
    "\n",
    "CTTransformer = model.punc_model\n",
    "\n",
    "# input: data_in(音频文件列表)\n",
    "# batch = {\n",
    "#                 \"feats\": speech,   [batch_size,  speech_length, feature_dimension]\n",
    "#                 \"waveform\": cache[\"frontend\"][\"waveforms\"],   [batch_size, sequence_length]\n",
    "#                 \"is_final\": kwargs[\"is_final\"],   True\n",
    "#                 \"cache\": cache,   \n",
    "#                 \"is_streaming_input\": is_streaming_input,\n",
    "#             }\n",
    "FsmnVADStreaming = model.vad_model   \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e115d4a99ea5eef1",
   "metadata": {},
   "source": [
    "res = model.generate(\n",
    "    # input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_vad_punc_example.wav\",\n",
    "    input=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav\",\n",
    "    batch_size_s=300,\n",
    "    batch_size_threshold_s=60,\n",
    ")\n",
    "print(res)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a8bbbb44",
   "metadata": {},
   "source": [
    "from funasr.models.bicif_paraformer.model import BiCifParaformer\n",
    "from funasr.train_utils.load_pretrained_model import load_pretrained_model\n",
    "from funasr.register import tables\n",
    "from funasr.utils.misc import deep_update\n",
    "\n",
    "kwargs = model.kwargs\n",
    "model_class = tables.model_classes.get(kwargs[\"model\"])\n",
    "model_conf = {}\n",
    "deep_update(model_conf, kwargs.get(\"model_conf\", {}))\n",
    "deep_update(model_conf, kwargs)\n",
    "bimodel_new = model_class(**model_conf, vocab_size=512)\n",
    "\n",
    "# bimodel = torch.load(\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\")\n",
    "load_pretrained_model(\n",
    "                    model=bimodel_new,\n",
    "                    path=\"iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\",\n",
    "                    ignore_init_mismatch=model.kwargs.get(\"ignore_init_mismatch\", True),\n",
    "                    oss_bucket=model.kwargs.get(\"oss_bucket\", None),\n",
    "                    scope_map=model.kwargs.get(\"scope_map\", []),\n",
    "                    excludes=model.kwargs.get(\"excludes\", None),\n",
    "                )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "584f9026cfedd5da",
   "metadata": {},
   "source": [
    "sequence_length = 218\n",
    "speech = torch.randn([1, sequence_length, 560])\n",
    "speech_lengths = torch.Tensor([sequence_length])\n",
    "bimodel = bimodel_new.to(\"cpu\")\n",
    "inputs = {\"speech\": speech, \"speech_lengths\": speech_lengths}\n",
    "torch.onnx.export(bimodel, inputs, \"BiCifParaformer.onnx.pb\", export_params=True, \n",
    "                  input_names=[\"speech\", \"speech_lengths\"],\n",
    "                  output_names=[\"results\"],\n",
    "                  dynamic_axes={'speech' : {1 : 'sequence_length'}})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c58b70329955bd7a",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b5f5616446d0c3a8",
   "metadata": {},
   "source": [
    "import onnxruntime\n",
    "\n",
    "x = torch.randn([1, 128, 560])\n",
    "ort_session = onnxruntime.InferenceSession(\"BiCifParaformer.onnx.pb\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x), ort_session.get_inputs()[1].name: to_numpy(torch.Tensor([128]))}\n",
    "\n",
    "# ort_inputs = {ort_session.get_inputs()[0], \"speech_lengths\": to_numpy(torch.Tensor([128]))}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(ort_outs)\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4d3e469f2da6e098",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
